{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417ad1d7",
   "metadata": {},
   "source": [
    "# <center> Topic 3. Statistical Learning with Scikit-Learn\n",
    "\n",
    "Сегодня вам предстоит на практике познакомиться с основными задачами машинного обучения.\n",
    "В ходе работы получится поработать с популярными библиотеками [`pandas`](https://pandas.pydata.org/), [`numpy`](https://numpy.org/), [`sklearn`](https://scikit-learn.org/stable/).\n",
    "\n",
    "Для выполнения задания необходимо следовать по этой тетрадке сверху вниз и заполнять недостающее части кода или отвечать на заданные вопросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbf3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки\n",
    "# Полезно все импорты держать рядом\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd56ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зафиксируем сид для генератора случайных чисел\n",
    "# Это полезно для воспроизводимости результатов\n",
    "\n",
    "RANDOM_SEED = 0xC0FFEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a33af6",
   "metadata": {},
   "source": [
    "# Классификация\n",
    "\n",
    "Для знакомства с задачей классификацией воспользуемся выборкой данных о пациентах с доброкачественными и злокачественными опухолями. Наша задача — научиться их отличать.\n",
    "\n",
    "Вместе с тетрадкой находится файл `cancer.csv` — это таблица, где каждая строчка соответствует отдельной клетке, а столбцы ее численные характеристики. Подробнее про датасет можно прочитать, например, вот [тут](\n",
    "https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data).\n",
    "\n",
    "Начнем с чтения данных с диска, для этого реализуйте функцию `read_cancer_dataset`.\n",
    "Поможет с этим библиотека `pandas` и пара полезных вещей из нее:\n",
    "1. [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n",
    "2. [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ca7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cancer_dataset(path_to_csv: str, shuffle: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Функция для чтения данных с диска, а также их случайного перемешивания\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_csv: Путь к файлу cancer.csv\n",
    "    shuffle: Если True, то перемешивает данные\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    dataframe: Данные в формате DataFrame\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "    if shuffle:\n",
    "        # .reset_index(drop=True)\n",
    "        df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5762dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на наши данные:\n",
    "# Колонка \"label\" отвечает за тип опухоли\n",
    "# Колонки 1-30 отвечают за признаки\n",
    "\n",
    "cancer_dataset = read_cancer_dataset(\"cancer.csv\", shuffle=True)\n",
    "cancer_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6a8cf",
   "metadata": {},
   "source": [
    "Первым делом необходимо подготовить данные к работе, а именно: разбить на тренировочную и тестовую части.\n",
    "\n",
    "<u>Тренировочная часть</u> используется для обучения моделей, именно по ней ищутся необходимые зависимости в данных.\n",
    "\n",
    "<u>Тестовая часть</u> используется для оценки качества моделей. Это данные, которые модель не видела, поэтому качество предсказаний по ним позволит оценить ее обобщающие способности.\n",
    "\n",
    "Крайне важно, чтобы тестовая и тренировочная части описывали одинаковую природу данных. Например, в случае задачи классификации, важно чтобы соотношение классов было приблизительно равно в них. Иначе мы можем неправильно интерпретировать результаты.\n",
    "\n",
    "Реализуйте функцию `prepare_cancer_dataset`, которая разделяет данные на таргет и признаки, а также выделяет тестовую часть. В этом может помочь [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) из библиотеки `sklearn`. Не забывайте фиксировать `random_state` или другие аналогичные параметры — это полезная привычка, которая съэкономить вам сотни часов дебага в будущем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9206d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cancer_dataset(\n",
    "    dataset: pd.DataFrame,\n",
    "    label_col_name: str = \"label\", \n",
    "    test_size: float = 0.1\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Функция для выделения таргета и признаков,\n",
    "    а также разделения на тренировочную и тестовую части.\n",
    "\n",
    "    Для таргета необходимо привести данные к формату 0/1.\n",
    "    Сопоставьте 0 доброкачественной опухоли (\"B\"),\n",
    "    а 1 злокачественной (\"M\")\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: DataFrame с датасетом\n",
    "    label_col_name: Название колонки с таргетом\n",
    "    test_size: доля тестовой выборки относительно всего датасета\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        4 numpy массива: X_train, X_test, y_train, y_test\n",
    "        X_train, X_test -- матрицы признаков размером [n_elements; 30]\n",
    "        y_train, y_test -- массивы из 0 и 1 размером [n elements]\n",
    "    \"\"\"\n",
    "    dataset[label_col_name] = dataset[label_col_name].replace({'M': 1, 'B': 0}).astype(int)\n",
    "    X = dataset.drop(columns=[label_col_name])\n",
    "    y = dataset[label_col_name]\n",
    "\n",
    "    split_kwargs = {\"test_size\": test_size, \"shuffle\": True, \"random_state\": RANDOM_SEED, \"stratify\": y} \n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, **split_kwargs) \n",
    "\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполним подготовку данных\n",
    "X_train, X_test, y_train, y_test = prepare_cancer_dataset(cancer_dataset)\n",
    "\n",
    "# Код ниже проверяет правильность подготовки данных\n",
    "# Если он упал, то надо исправить функцию выше\n",
    "assert X_train.shape == (512, 30) and y_train.shape == (512,)\n",
    "assert X_test.shape == (57, 30) and y_test.shape == (57,)\n",
    "\n",
    "train_ratio = y_train.sum() / len(y_train)\n",
    "test_ratio = y_test.sum() / len(y_test)\n",
    "assert train_ratio < 0.5\n",
    "assert np.abs((test_ratio - train_ratio) / train_ratio) < 0.015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61401b13",
   "metadata": {},
   "source": [
    "Начнем с наивного решения — модель, которая предсказывает наиболее популярный класс.\n",
    "Реализуйте методы `fit` и `predict` у класса ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d68353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class MostCommonClassifier:\n",
    "    def __init__(self):\n",
    "        self.predict_class = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Функция обучения наивной модели.\n",
    "        Она получает на вход X и y,\n",
    "        чтобы иметь схожий интерфейс с другими моделями.\n",
    "\n",
    "        Функция определяет самый популярный класс и\n",
    "        сохраняет его в predict_class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: признаки, не используются\n",
    "        y: таргет, номера классов, одномерный массив\n",
    "        \"\"\"\n",
    "        # or\n",
    "        # unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        # most_common_index = np.argmax(counts)\n",
    "        # self.predict_class = unique_classes[most_common_index]\n",
    "  \n",
    "        y_counter = Counter(y)\n",
    "        self.predict_class = max(y_counter, key=lambda k: y_counter[k])\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Функция для предсказания классов\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: элементы, для которых надо предсказать класс\n",
    "            матрица размером [n_elements; n_features]\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "            предсказанный класс для каждого элемента\n",
    "            numpy массив размером [n_elements]\n",
    "        \"\"\"\n",
    "        if self.predict_class is None:\n",
    "            raise RuntimeError(\"Call fit before predict\")\n",
    "        return np.full(shape=(X.shape[0],), fill_value=self.predict_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c2fba",
   "metadata": {},
   "source": [
    "\"Обучим\" наивную модель и оценим ее качество.\n",
    "\n",
    "Для оценки качества воспользуемся двумя популярными метриками:\n",
    "1. Точность (accuracy) измеряет, как часто модель предсказывает правильные ответы из всех возможных ответов. Она вычисляется как отношение числа правильных предсказаний к общему числу предсказаний. Например, если модель правильно предсказала 80 из 100 объектов, то точность будет равна 0.8 или 80%.\n",
    "\n",
    "2. F1-score — более сложная метрика, она измеряет сбалансированность модели, учитывая как точность (precision), так и полноту (recall) предсказаний. Точнее говоря, она считает их гармоническое среднее. Использование такой метрики позволяет более точно оценить модели в случае сильной несбалансированности в данных.\n",
    "\n",
    "Более подробно ознакомиться с метриками классификации можно, например, [тут](https://vk.com/@itresume-sobaka-ili-koshka-razbor-precision-recall-i-f1-score-i-drug)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2705a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\", f\"F1-score: {f1 * 100:.2f}%\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ee25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_most_common = MostCommonClassifier()\n",
    "model_most_common.fit(X_train, y_train)\n",
    "y_pred_most_common = model_most_common.predict(X_test)\n",
    "\n",
    "print_classification_report(y_test, y_pred_most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe2e46",
   "metadata": {},
   "source": [
    "Один из самых простых алгоритмов классификации —  это \"K ближайших соседей\".\n",
    "В нем для каждого объекта находятся k наиболее близких объектов и выбирается самый частотный класс среди них.\n",
    "Давайте воспользуемся им для решения нашей задачи.\n",
    "Поможет в этом реализация из `sklearn`: [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = KNeighborsClassifier(n_jobs=-1)\n",
    "model_v1.fit(X_train, y_train)\n",
    "y_pred_v1 = model_v1.predict(X_test)\n",
    "\n",
    "print_classification_report(y_test, y_pred_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce63b28",
   "metadata": {},
   "source": [
    "Результат уже стал значительно выше! Если вы все сделали верно, то уже должны получить точность выше 90%.\n",
    "\n",
    "Однако еще есть куда расти.\n",
    "Один из главных способов поднять качество — это правильно настроить модель.\n",
    "\n",
    "Ознакомьтесь с документацией алгоритма по ссылке выше и поиграйтесь с параметрами модели.\n",
    "Например, вместо стандартного `n_neighbors=5` можно поставить `n_neighbors=7`.\n",
    "Тогда при предсказании класса модель будет смотреть не на 5 ближайших соседей, а на 7.\n",
    "\n",
    "Попробуйте получить как можно более высокое качество!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = KNeighborsClassifier(n_neighbors=2, n_jobs=-1)\n",
    "model_v2.fit(X_train, y_train)\n",
    "y_pred_v2 = model_v2.predict(X_test)\n",
    "# print_classification_report(y_test, y_pred_v2)\n",
    "\n",
    "my_dict = {\"k\":0, \"accuracy\": 0, \"f1\":0}\n",
    "for k in range(1, 51):\n",
    "    model_v2_1 = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    model_v2_1.fit(X_train, y_train)\n",
    "    y_pred_v2_1 = model_v2_1.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_v2_1)\n",
    "    f1 = f1_score(y_test, y_pred_v2_1)\n",
    "\n",
    "    if accuracy > my_dict[\"accuracy\"]:\n",
    "        my_dict[\"accuracy\"] = accuracy\n",
    "        my_dict[\"k\"] = k\n",
    "    if f1 > my_dict[\"f1\"]:\n",
    "        my_dict[\"f1\"] = f1\n",
    "\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96dd6f",
   "metadata": {},
   "source": [
    "Одна из особенностей алгоритма \"K ближайших соседей\" — это необходимость вычислять расстояние между векторами признаков. По умолчанию используется обычное евклидово расстояние:\n",
    "\n",
    "$$\n",
    "\\text{dist}(p, q) = \\sqrt{\\sum_{i=1}^n (p_i^2 - q_i^2)}\n",
    "$$\n",
    "\n",
    "Здесь $p$ и $q$ — это вектора размерности $n$, то есть массивы, описывающие $n$ признаков.\n",
    "\n",
    "Из формулы можно заменить, что если значения одного из признаков очень большие, то он будет подавлять вклад признаков с маленькими значениями.\n",
    "\n",
    "Давайте посмотрим на средние значения каждого признака в нашем датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.grid(visible=True)\n",
    "pyplot.yscale(\"log\")\n",
    "pyplot.plot(X_train.mean(axis=0))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c80b37",
   "metadata": {},
   "source": [
    "Можно заметить, что некоторые признаки в среднем варьируются возле 1000, тогда как другие меньше 0.01.\n",
    "\n",
    "Чтобы это исправить можно отмасштабировать признаки, а именно привести каждый признак к среднему 0 и дисперсии 1.\n",
    "Помочь в этом может [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) из библиотеки `sklearn`.\n",
    "\n",
    "Изучите документацию этого алгоритма и реализуйте функцию `scale_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f54087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(\n",
    "    train_data: np.ndarray, test_data: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Функция для масштабирования данных\n",
    "    Переводит каждый столбец данных в новый со средним 0 и дисперсией 1.\n",
    "\n",
    "    Для подсчета статистики и обучения используется тренировочная часть.\n",
    "    Затем масштабирование применяется и к тестовым данным.\n",
    "\n",
    "    Parameters\n",
    "        ----------\n",
    "        train_data: матрица размером [train_size; n_features]\n",
    "            Тренировочная часть\n",
    "        test_data: матрица размером [test_size; n_features]\n",
    "            Тестовая часть\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "            train_data_scaled, test_data_scaled\n",
    "                numpy матрицы с отмасштабированными данными\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    train_data_scaled = scaler.fit_transform(train_data)\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "    return train_data_scaled, test_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aedde67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled = scale_features(X_train, X_test)\n",
    "\n",
    "mean, std = X_train_scaled.mean(axis=0), X_train_scaled.std(axis=0)\n",
    "assert np.allclose(mean, 0) and np.allclose(std, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b6480",
   "metadata": {},
   "source": [
    "Обучите `KNeighborsClassifier` на новых данных, не забудьте подобрать оптимальные гиперпараметры.\n",
    "Возможно достить точности выше 95%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v3 = KNeighborsClassifier(n_neighbors=11, n_jobs=-1)\n",
    "model_v3.fit(X_train_scaled, y_train)\n",
    "y_pred_v3 = model_v3.predict(X_test_scaled)\n",
    "\n",
    "print_classification_report(y_test, y_pred_v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f40236",
   "metadata": {},
   "source": [
    "Задачу классификации можно решать множеством разных способов, многие из которых реализованы в библиотеки `sklearn`.\n",
    "\n",
    "Вы можете ознакомиться со всем списком алгоритмов в библиотеке [здесь](https://scikit-learn.org/stable/supervised_learning.html). Не все они подходят для задачи классификации, ориентируйтесь на слово `Classifier` в названии, а также не стесняйтесь переходить по ссылкам и читать документацию и описание.\n",
    "\n",
    "Попробуйте применить новые алгоритмы к нашей задаче. Рекомендуем обратить внимание на:\n",
    "1. [`LogisticRegression`](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "2. [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)\n",
    "3. [`SVC`](https://scikit-learn.org/stable/modules/svm.html#classification)\n",
    "\n",
    "Для методов на основе линейных преобразований полезно использовать отмасштабированные данные.\n",
    "\n",
    "Вполне реально получить идеальное качество в 100%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf82992",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_1 = LogisticRegression()\n",
    "model_1.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_model_1 = model_1.predict(X_test_scaled)\n",
    "\n",
    "print_classification_report(y_test, y_pred_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497367a4",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_2 = RandomForestClassifier()\n",
    "model_2.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_model_2 = model_2.predict(X_test_scaled)\n",
    "\n",
    "print_classification_report(y_test, y_pred_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d649ea4",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_3 = SVC(C=1)\n",
    "model_3.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_model_3 = model_3.predict(X_test_scaled)\n",
    "\n",
    "print_classification_report(y_test, y_pred_model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37cb02",
   "metadata": {},
   "source": [
    "# Регрессия\n",
    "\n",
    "Для знакомства с задачей регрессии, нам поможет популярный датасет [Boston](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), он прикреплен к заданию в файле `boston.csv`.\n",
    "Это набор данных с информаций о медианной стоимости домов, а также различных характеристик района.\n",
    "Ознакомиться с датасетом можно по ссылке выше, а ниже представленно описание каждого столбца в данных:\n",
    "```\n",
    "1. crim      per capita crime rate by town\n",
    "2. zn        proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. indus     proportion of non-retail business acres per town\n",
    "4. chas      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. nox       nitric oxides concentration (parts per 10 million)\n",
    "6. rm        average number of rooms per dwelling\n",
    "7. age       proportion of owner-occupied units built prior to 1940\n",
    "8. dis       weighted distances to five Boston employment centres\n",
    "9. rad       index of accessibility to radial highways\n",
    "10. tax      full-value property-tax rate per \\$10,000\n",
    "11. ptratio  pupil-teacher ratio by town\n",
    "12. b        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. lstat    % lower status of the population\n",
    "14. medv     median value of owner-occupied homes in \\$'s\n",
    "```\n",
    "\n",
    "Наша задача — научится предсказывать стоимость дома по критериям района. То есть вместо ограниченного числа значений, модель теперь должна предсказывать любые целые числа.\n",
    "\n",
    "Начнем с функции `read_boston_dataset`, которая считывает датасет с диска. В данных первые 14 строчек не относятся к данным, а описывают колонки, для их пропуска полезно использовать `skiprows` в функции `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b516a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_boston_dataset(path_to_csv: str, shuffle: bool = True) -> np.ndarray:\n",
    "    \"\"\"Функция для чтения данных с диска, а также их случайного перемешивания\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_csv: Путь к файлу boston.csv\n",
    "    shuffle: Если True, то перемешивает данные\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    dataframe: Данные в формате DataFrame\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(path_to_csv, comment='#')\n",
    "    if shuffle:\n",
    "        dataframe = dataframe.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eecb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_dataset = read_boston_dataset(\"boston.csv\")\n",
    "boston_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на данные чуть ближе\n",
    "# Оценим распределение цен в датасете\n",
    "\n",
    "pyplot.hist(boston_dataset[\"medv\"])\n",
    "pyplot.grid(visible=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac440f",
   "metadata": {},
   "source": [
    "**Вопрос:** сделайте 2-3 вывода относительно цен.\n",
    "\n",
    "1. основная часть расположена в диапазоне от 10_000 до 30_000\n",
    "2. весомая доля дорогих цен на жилье (тяжелый правый хвост)\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e0daa",
   "metadata": {},
   "source": [
    "По аналогии с задачей классификацией, необходимо выделить тренировочную и тестовую выборку.\n",
    "Реализуйте для этого функцию `prepare_boston_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58335c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_boston_dataset(\n",
    "    dataset: pd.DataFrame, label_col_name: str = \"medv\", test_size: float = 0.1\n",
    ") -> tuple[np.ndarray, ...]:\n",
    "    \"\"\"Функция для выделения таргета и признаков,\n",
    "    а также разделения на тренировочную и тестовую части.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: DataFrame с датасетом\n",
    "    label_col_name: Название колонки с таргетом\n",
    "    test_size: доля тестовой выборки относительно всего датасета\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        4 numpy массива: X_train, X_test, y_train, y_test\n",
    "        X_train, X_test -- матрицы признаков размером [n_elements; 13]\n",
    "        y_train, y_test -- массивы с ценами размером [n elements]\n",
    "    \"\"\"\n",
    "    X = dataset.drop(columns=[label_col_name])\n",
    "    y = dataset[label_col_name]\n",
    "    split_params = {\"test_size\": test_size, \"random_state\": RANDOM_SEED}\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, **split_params)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b93df2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_boston_dataset(boston_dataset)\n",
    "\n",
    "# Код ниже проверяет правильность подготовки данных\n",
    "# Если он упал, то надо исправить функцию выше\n",
    "\n",
    "assert X_train.shape == (455, 13) and y_train.shape == (455,)\n",
    "assert X_test.shape == (51, 13) and y_test.shape == (51,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c91dc",
   "metadata": {},
   "source": [
    "Аналогично задаче классификации, начнем с наивного решения. Для задаче регрессии можно использовать, например, среднее значение по датасету. Однако вы можете предложить и свою оценку на основе анализа графика и вывода выше.\n",
    "\n",
    "Реализуйте методы `fit` и `predict` у класса ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ae1a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanRegression:\n",
    "    def __init__(self):\n",
    "        self.mean_value = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Функция обучения наивной модели.\n",
    "        Она получает на вход X и y,\n",
    "        чтобы иметь схожий интерфейс с другими моделями.\n",
    "\n",
    "        Функция определяет среднюю величину таргета\n",
    "        и сохраняет его в mean_value\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: признаки, не используются\n",
    "        y: таргет, целые числа, одномерный массив\n",
    "        \"\"\"\n",
    "        self.mean_value = y.mean()\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Функция для предсказания классов\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: элементы, для которых надо предсказать значение\n",
    "            матрица размером [n_elements; n_features]\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "            предсказанные значения для каждого элемента\n",
    "            numpy массив размером [n_elements]\n",
    "        \"\"\"\n",
    "        if self.mean_value is None:\n",
    "            raise RuntimeError(\"Call fit before predict\")\n",
    "        return np.full(shape=(X.shape[0],), fill_value=self.mean_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8a3b2",
   "metadata": {},
   "source": [
    "Обучим \"наивную\" модель и оценим ее качество.\n",
    "\n",
    "В задаче регресии также существует большое множество метрик. Ознакомиться с ними можно, например, [тут](https://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D1%86%D0%B5%D0%BD%D0%BA%D0%B0_%D0%BA%D0%B0%D1%87%D0%B5%D1%81%D1%82%D0%B2%D0%B0_%D0%B2_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0%D1%85_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8_%D0%B8_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%B8).\n",
    "\n",
    "В нашем случае мы также будем использовать две метрики:\n",
    "1. MSE (Mean Squared Error) — среднее квадратичное отклонение, интуитивно понятная метрика, но не всегда хорошо интерпретируется.\n",
    "\n",
    "2. R2-score — \"нормированная\" MSE, не имеет границы снизу, 0 в случае предсказания среднего значения и 1 для идеальной работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e62a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_regression_report(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"MSE: {mse:.2f}\", f\"R2-score: {r2:.2f}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean = MeanRegression()\n",
    "model_mean.fit(X_train, y_train)\n",
    "y_pred_mean = model_most_common.predict(X_test)\n",
    "\n",
    "print_regression_report(y_test, y_pred_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa371c1a",
   "metadata": {},
   "source": [
    "Получили низкое качество, поэтому перейдем к более серьезным моделям.\n",
    "\n",
    "Одна из них — это линейная регрессия. Интутивно простая модель, но крайне выразительно и часто применяющаяся в различных вариациях и модификациях. Линейная регрессия предпологает линейную зависимость между признаками и таргетами и описывается следующей формулой:\n",
    "\n",
    "$$\n",
    "y = w_1 * x_1 + w_2 * x_2 + \\dots + w_n + x_n + b = \\sum_{i=1}^n w_i x_i + b\n",
    "$$\n",
    "\n",
    "Здесь $y$ — это таргет, $x_1, \\dots x_n$ — признаки, а $w_1, \\dots, w_n$ и $b$ — <u>параметры</u> модели.\n",
    "\n",
    "В ходе тренировки модели эти параметры автоматически подбираются под обучающие данные.\n",
    "\n",
    "Конечно, предполагать линейную зависимость между признаками и таргетом во многих случаях бывает невозможно. Для этого можно воспользоваться специальными ядрами, нелинейными преобразованиями, для обработки данных, созданием [полиномиальных признаков](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) или воспользоваться одной из [модификацией](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) уже реализованной в `sklearn`.\n",
    "\n",
    "Обучим [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) из `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = LinearRegression(n_jobs=-1)\n",
    "model_v1.fit(X_train, y_train)\n",
    "y_pred_v1 = model_v1.predict(X_test)\n",
    "\n",
    "print_regression_report(y_test, y_pred_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea71f12",
   "metadata": {},
   "source": [
    "Качество модели значительно лучше.\n",
    "\n",
    "Давайте проанализируем полученную модель, а именно посмотрим какие веса $w$ получились для каждого признака:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = boston_dataset.columns[:-1]\n",
    "coefs = model_v1.coef_\n",
    "\n",
    "for name, cf in sorted(zip(feature_names, coefs), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}\\t{cf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01504eaa",
   "metadata": {},
   "source": [
    "**Вопрос:** как можно интерпретировать полученный список?\n",
    "\n",
    "**Ответ:** \n",
    "- АБСОЛЮТНЫЕ ЗНАЧЕНИЯ: наиболее важный признак - число комнат, наименее важный - концентрация оксидов азота\n",
    "- УЧЕТ ЗНАКА: если плюс, то итоговая цена растет; если минус то цена снижается (прямая и обратная пропорциональность)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08191c61",
   "metadata": {},
   "source": [
    "Внутри `sklearn` есть множество алгоритмов регрессии, попробуйте применить их для этой задачи.\n",
    "\n",
    "Например, можно взглянуть на [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), довольно мощный алгоритм, но требующий детальной настройки.\n",
    "\n",
    "В этой задаче можно получить R2-score больше 0.9, удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb838003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_v2 = GradientBoostingRegressor(loss=\"squared_error\", max_depth=4, random_state=RANDOM_SEED)\n",
    "\n",
    "def scale(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "print(\"===============WITHOUT SCALE================\")\n",
    "model_v2.fit(X_train, y_train)\n",
    "y_pred_v2 = model_v2.predict(X_test)\n",
    "print_regression_report(y_test, y_pred_v2)\n",
    "print(\"=================WITH SCALE=================\")\n",
    "X_train_scaled_boston, X_test_scaled_boston = scale(X_train, X_test)\n",
    "model_v2.fit(X_train_scaled_boston, y_train)\n",
    "y_pred_v2 = model_v2.predict(X_test_scaled_boston)\n",
    "print_regression_report(y_test, y_pred_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33f1dc",
   "metadata": {},
   "source": [
    "# Кластеризация\n",
    "\n",
    "Последний блок нашей практики посвящен задаче кластеризации, задаче где отсутствуют таргеты и необходимо уметь группировать данные в осмысленные блоки. Примерами задачи кластеризации может служить разбиение новостей по разным темам или выявление пользователей в соц. сетях с общими интересами.\n",
    "\n",
    "Мы применим кластеризацию к картинкам, что может быть полезно, если необходимо ее сжать.\n",
    "\n",
    "Выберите любую картинку, может быть любимый шаблон мема или чья-то фотография. Пример подходящей картинки прикреплен к практике, в файле `image.jpg`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f01c57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path: str) -> np.ndarray:\n",
    "    with Image.open(image_path) as img:\n",
    "        data = np.array(img)\n",
    "    return data\n",
    "\n",
    "\n",
    "def show_image(image: np.ndarray) -> np.ndarray:\n",
    "    pyplot.axis(\"off\")\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.imshow(image)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11159a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"image.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_image(IMAGE_NAME)\n",
    "height, width = image.shape[:2]\n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3dcf47",
   "metadata": {},
   "source": [
    "Картинка в памяти хранится как трехмерный массив `[h; w; 3]`, однако алгоритмы кластеризации требуют от нас двумерный массив `[n_samples; n_features]`. В случае картинок, `n_features` — 3, RGB код цвета каждого пикселя, а `n_samples` общее число пикселей.\n",
    "\n",
    "Реализуйте функцию `preprocess_image`, которая получает картинку и возвращает нужный двумерный массив."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5661273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Функция для препроцессинга картинки\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: исходная картинка\n",
    "        массив размером [h; w; 3]\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        матрица размером [n_pixels; 3]\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    return image.reshape(height*width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "692788e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_image(image)\n",
    "assert X_train.shape == (height * width, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cf326",
   "metadata": {},
   "source": [
    "В качестве первого алгоритма возьмем [`KMeans`](https://scikit-learn.org/stable/modules/clustering.html#k-means). Его идея близка к алгоритму классификации \"K ближайших соседей\", считаются попарные расстояния между точками и наиболее близкие объединяются в кластеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb672e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=5, n_init=1, random_state=RANDOM_SEED)\n",
    "k_means.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e8b67",
   "metadata": {},
   "source": [
    "Заменим каждый цвет на картинке на средний цвет кластера, куда попал соответствующий кластер.\n",
    "\n",
    "Для этого реализуйте функцию `replace_to_centroid`, которая принимает полученные индексы кластеров и цвета кластеров и возвращает цвета для каждой точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d057df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_to_centroid(\n",
    "    predicted_cluster: np.ndarray, centroids: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Функция для получения центроиды кластера по ее индексу\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_cluster: предсказанные кластеры\n",
    "        массив размером [n_samples]\n",
    "        каждое значение от 0 до n_clusters\n",
    "    centroids: центры кластеров\n",
    "        массив размером [n_clusters; 3]\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        матрица размером [n_samples; 3]\n",
    "    \"\"\"\n",
    "\n",
    "    # return centroids[predicted_cluster]\n",
    "    return np.array([centroids[colour] for colour in predicted_cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "445c670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clusters = k_means.predict(X_train)\n",
    "X_predicted = replace_to_centroid(predicted_clusters, k_means.cluster_centers_)\n",
    "\n",
    "assert X_predicted.shape == X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f01dc",
   "metadata": {},
   "source": [
    "Приведем матрицу обратно к формату картинки и посмотрим, что получилось!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = X_predicted.reshape(height, width, 3).astype(np.int32)\n",
    "show_image(new_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2647d12",
   "metadata": {},
   "source": [
    "Изучите другие алгоритмы кластеризации, доступные в `sklearn`: [алгоритмы](https://scikit-learn.org/stable/modules/clustering.html).\n",
    "\n",
    "Для применения к нашей задаче, необходимо выбрать такой, где задается число кластеров, обычно параметр называется `n_clusters`. Помните, что задача кластеризации трудная с вычислительной точки зрения, домашний ПК не всегда может с ней справиться.\n",
    "\n",
    "Поэкспериментируйте с другими алгоритмами и сравните, как они ведут себя относительно `KMeans` для задачи сжатия изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a0b71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, HDBSCAN, OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d400ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3, n_jobs=-1)\n",
    "# cluster_model_1.fit(X_train)\n",
    "# print(sorted(Counter(cluster_model_1.labels_)))\n",
    "\n",
    "predicted_clusters = dbscan.fit_predict(X_train)\n",
    "unique_clusters = np.unique(predicted_clusters)\n",
    "centroids = np.array([X_train[predicted_clusters == cluster].mean(axis=0) for cluster in unique_clusters if cluster != -1])\n",
    "def replace_to_centroid(predicted_cluster: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
    "    return np.array([centroids[colour] if colour != -1 else np.zeros(3) for colour in predicted_cluster])\n",
    "\n",
    "X_predicted = replace_to_centroid(predicted_clusters, centroids)\n",
    "assert X_predicted.shape == X_train.shape\n",
    "assert X_predicted.shape == X_train.shape\n",
    "\n",
    "new_image = X_predicted.reshape(height, width, 3).astype(np.int32)\n",
    "show_image(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beaea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(min_cluster_size=2, n_jobs=-1)\n",
    "# cluster_model_1.fit(X_train)\n",
    "# print(sorted(Counter(cluster_model_1.labels_)))\n",
    "\n",
    "predicted_clusters = hdbscan.fit_predict(X_train)\n",
    "unique_clusters = np.unique(predicted_clusters)\n",
    "centroids = np.array([X_train[predicted_clusters == cluster].mean(axis=0) for cluster in unique_clusters if cluster != -1])\n",
    "def replace_to_centroid(predicted_cluster: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
    "    return np.array([centroids[colour] if colour != -1 else np.zeros(3) for colour in predicted_cluster])\n",
    "\n",
    "X_predicted = replace_to_centroid(predicted_clusters, centroids)\n",
    "assert X_predicted.shape == X_train.shape\n",
    "assert X_predicted.shape == X_train.shape\n",
    "\n",
    "new_image = X_predicted.reshape(height, width, 3).astype(np.int32)\n",
    "show_image(new_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cbe88",
   "metadata": {},
   "source": [
    "### TOOO LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "optics = OPTICS(min_samples=5, n_jobs=-1)\n",
    "\n",
    "predicted_clusters = optics.fit_predict(X_train)\n",
    "unique_clusters = np.unique(predicted_clusters)\n",
    "centroids = np.array([X_train[predicted_clusters == cluster].mean(axis=0) for cluster in unique_clusters if cluster != -1])\n",
    "def replace_to_centroid(predicted_cluster: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
    "    return np.array([centroids[colour] if colour != -1 else np.zeros(3) for colour in predicted_cluster])\n",
    "\n",
    "X_predicted = replace_to_centroid(predicted_clusters, centroids)\n",
    "assert X_predicted.shape == X_train.shape\n",
    "assert X_predicted.shape == X_train.shape\n",
    "\n",
    "new_image = X_predicted.reshape(height, width, 3).astype(np.int32)\n",
    "show_image(new_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
